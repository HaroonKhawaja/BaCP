Model : vgg11 - Learning Type: cifar10/bacp_LRS_adamw_0.0001/magnitude_pruning/0.95
Configuration:
model_name: vgg11
model_task: cifar10
model_type: cv
num_classes: 10
batch_size: 512
learning_rate: 0.0001
optimizer_type: adamw
epochs: 1
recovery_epochs: 1
patience: 1
pruning_type: magnitude_pruning
target_sparsity: 0.95
sparsity_scheduler: cubic
pruning_epochs: 1
n_views: 2
temperature: 20
base_temperature: 20
device: cuda
enable_mixed_precision: True
num_workers: 24
train_batches: 83
val_batches: 14
current_model_path: /dbfs/research/vgg11/cifar10/vgg11_cifar10_magnitude_pruning_0.95_bacp_LRS_adamw_0.0001.pt

Epoch [1/1]: Avg Total Loss: 7.4683 | Avg PrC Loss: 3.4643 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 3.4629 | Avg CE Loss: 0.5411 | Model Sparsity: 0.95
Retraining Epoch [1/1]: Avg Total Loss: 10.7580 | Avg PrC Loss: 3.4626 | Avg SnC Loss: 3.4493 | Avg FiC Loss: 3.4595 | Avg CE Loss: 0.3866 | Model Sparsity: 0.95
