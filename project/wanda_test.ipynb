{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a9ae51-88d5-4a44-9724-222fb310ae08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bde51b7-8be0-49a2-8664-fdb723e846b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install torchinfo\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d864be54-f8b5-4448-b908-2f110b1e7fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "# from contrastive_learning import ContrastiveLearner\n",
    "# from models import EncoderProjectionNetwork, ClassificationNetwork\n",
    "# from datasets_class import CreateDatasets\n",
    "# from supervised_learning import train, test\n",
    "from bacp import BaCPLearner, BaCPTrainer, BaCPTrainingArgumentsLLM\n",
    "from models import EncoderProjectionNetwork, ClassificationNetwork\n",
    "from unstructured_pruning import MagnitudePrune, MovementPrune, LocalMagnitudePrune, LocalMovementPrune, WandaPrune, PRUNER_DICT, check_model_sparsity\n",
    "from LLM_trainer import LLMTrainer, LLMTrainingArguments\n",
    "from dataset_utils import get_glue_data\n",
    "from logger import Logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/dbfs/hf_datasets\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from utils import *\n",
    "from constants import *\n",
    "\n",
    "device = get_device()\n",
    "print(f\"{device = }\")\n",
    "BATCH_SIZE_DISTILBERT = 64\n",
    "NUM_WORKERS = 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ccfbfc-6204-4ca3-b707-7ad645dcce8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Baseline Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0596a87d-a66b-4c9c-8227-de804c766dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DistilBERT Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "364d9865-e044-4ee7-b231-1a29a8b076e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QQP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "866edd0e-5abd-42f1-8665-beeee7048c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 3\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "model_task = \"qqp\"\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainset_qqp, valset_qqp, testset_qqp = data[\"trainset\"], data[\"valset\"], data[\"testset\"]\n",
    "trainloader_qqp, valloader_qqp, testloader_qqp = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='baseline_accuracies',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader_qqp,\n",
    "    valloader=valloader_qqp,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c0e00f-1cd4-4e81-9dc6-89b4a054ba6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SST-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a2f932-cae2-42a3-9e5d-c602bcb847c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Model initialization\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# # Hyperparameter initialization\n",
    "# learning_rate = 2e-5\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# epochs = 3\n",
    "# weight_decay = 0.01\n",
    "\n",
    "# # Data initialization\n",
    "# model_task = \"sst2\"\n",
    "# data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "# trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "\n",
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    epochs=3,\n",
    "    learning_type=\"baseline\",\n",
    "    prune=False,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44166101-00f2-4d4e-a09b-cce4e123c234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RoBERTa Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fdd1f44-c72a-4f7d-bde2-d799eb75bdde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### QQP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b2844c-f82f-4364-ac20-6dcdfebaddc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 3\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "model_task = \"qqp\"\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainset_qqp, valset_qqp, testset_qqp = data[\"trainset\"], data[\"valset\"], data[\"testset\"]\n",
    "trainloader_qqp, valloader_qqp, testloader_qqp = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='baseline_accuracies',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader_qqp,\n",
    "    valloader=valloader_qqp,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ef90a6e-a5cd-49fe-9018-b73ce51ff883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SST-2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0158f2d-bc53-488f-9a45-9fe163bd9295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 3\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "model_task = \"sst2\"\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainset_qqp, valset_qqp, testset_qqp = data[\"trainset\"], data[\"valset\"], data[\"testset\"]\n",
    "trainloader_qqp, valloader_qqp, testloader_qqp = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='baseline_accuracies',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader_qqp,\n",
    "    valloader=valloader_qqp,\n",
    ")\n",
    "\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7be69d1d-373f-4b12-a48d-9ab5140e1728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Pruning Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63ac29f0-e096-416d-a2de-ad111a4447e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DistilBERT Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6555eb17-e586-47bd-823d-494c738fb285",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SST-2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d58a036-4334-44c0-823e-031ae7e08669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Magnitude Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98519d4-561a-4ecd-9fc3-9c5394efc5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    \n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defe99f3-fbdb-4502-b1a1-d11ce121da08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ba3b50-f1da-447b-9cb2-909914a423a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4d1270a-6f28-4e85-8416-c473016644cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Movement Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab38d80e-18d7-4212-a829-d6d300d611cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d511df92-bbae-4f32-88be-368dab9b0d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44235e91-261e-4573-ba7a-e422171d8e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b8b53cd-9626-4b42-8655-6cfe9cf07194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### WandA Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4ffcd4-02dd-41d9-9870-36e12b20926f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79cfef5e-ce9c-40e6-873f-05e90d5c2ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a21b89-e8aa-4377-a402-7e1cd7f66155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "\n",
    "training_args = LLMTrainingArguments(\n",
    "    pruning_type=pruning_type,\n",
    "    target_sparsity=target_sparsity,\n",
    "    pruning_scheduler=\"cubic\",\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='pruning',\n",
    "    learning_rate=1e-5,\n",
    "    epochs=50,\n",
    "    recovery_epochs=0,\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(training_args=training_args)\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e2a26e2-3b6a-4d20-baf5-c67793baa171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "for data in trainloader:\n",
    "    model(input_ids=data[\"input_ids\"].to(device), attention_mask=data[\"attention_mask\"].to(device), labels=data[\"label\"].to(device))\n",
    "    break\n",
    "\n",
    "\n",
    "pruner.prune(model)\n",
    "pruner.apply_mask(model)\n",
    "print(check_sparsity(model))\n",
    "\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n",
    "\n",
    "for i in range(epochs-1):\n",
    "    pruner.ratio_step()\n",
    "    pruner.prune(model)\n",
    "    pruner.apply_mask(model)\n",
    "    print(check_sparsity(model))\n",
    "\n",
    "    acc = trainer.evaluate()\n",
    "    print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a608b34-7c5c-48a2-8d04-e14277111e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RoBERTa Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72b3d025-a65c-41c4-8839-e776cf9f2191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SST-2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c66fa4-48de-4ebe-8883-6c46d4871c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Magnitude Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72ea36e8-c31a-48f8-ac12-1498038f3185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "pruner = MagnitudePrune(epochs, TARGET_SPARSITY_LOW)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_LOW}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_LOW\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34cc3944-8157-4d8c-8b89-251ffa50a35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "pruner = MagnitudePrune(epochs, TARGET_SPARSITY_MID)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_MID}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_MID\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89463740-1ca5-4aff-8370-83c18e45fc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"magnitude_pruning\"\n",
    "pruner = MagnitudePrune(epochs, TARGET_SPARSITY_HIGH)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_HIGH}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "854e6799-4036-41f1-b123-2c03301c72ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Movement Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9308e30d-0972-4b8f-8207-2bf73957a6a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "pruner = MovementPrune(epochs, TARGET_SPARSITY_LOW)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_LOW}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_LOW\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d4b94f-6be9-42b2-984b-e6e54149595e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "pruner = MovementPrune(epochs, TARGET_SPARSITY_MID)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_MID}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_MID\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6caaab1-3a8e-419d-a1db-b095a8be42f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"movement_pruning\"\n",
    "pruner = MovementPrune(epochs, TARGET_SPARSITY_HIGH)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_HIGH}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087bc1bb-603e-4a8a-92ed-c3794b58e3a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Wanda Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfdf1b59-073e-4b7c-8ef6-d5993ffa336d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "pruner = WandaPrune(epochs, TARGET_SPARSITY_LOW, model)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_LOW}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_LOW\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef73b302-c2b9-44aa-aa23-5b487ec96596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "pruner = WandaPrune(epochs, TARGET_SPARSITY_MID, model)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_MID}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_MID\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53e9acf-af03-412f-bb76-5d2313efd8ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "model_name = \"roberta-base\"\n",
    "model_task = \"sst2\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Loading trained weights\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pth\"\n",
    "model.load_state_dict(torch.load(trained_model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Hyperparameter initialization\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "weight_decay = 0.01\n",
    "\n",
    "# Data initialization\n",
    "data = get_glue_data(model_name, tokenizer, model_task, BATCH_SIZE_DISTILBERT, NUM_WORKERS)\n",
    "trainloader, valloader, testloader = data[\"trainloader\"], data[\"valloader\"], data[\"testloader\"]\n",
    "\n",
    "# Initializing pruning method\n",
    "pruning_type = \"wanda_pruning\"\n",
    "pruner = WandaPrune(epochs, TARGET_SPARSITY_HIGH, model)\n",
    "recovery_epochs = 5\n",
    "\n",
    "save_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_{pruning_type}_{TARGET_SPARSITY_HIGH}.pth\"\n",
    "training_args = LLMTrainingArguments(\n",
    "    model_name=model_name,\n",
    "    model_task=model_task,\n",
    "    learning_rate=learning_rate,\n",
    "    epochs=epochs,\n",
    "    batch_size=BATCH_SIZE_DISTILBERT,\n",
    "    optimizer=optimizer,\n",
    "    weight_decay=weight_decay,\n",
    "    learning_type='pruning',\n",
    "    log_epochs=True,\n",
    "    enable_tqdm=True,\n",
    "    enable_mixed_precision=True,\n",
    "    save_path = save_path,\n",
    "    pruner=pruner,\n",
    "    pruning_type=pruning_type,\n",
    "    recovery_epochs=recovery_epochs,\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH\n",
    ")\n",
    "\n",
    "trainer = LLMTrainer(\n",
    "    model=model,\n",
    "    training_args=training_args,\n",
    "    trainloader=trainloader,\n",
    "    valloader=valloader,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    trainer.train()\n",
    "\n",
    "model.load_state_dict(torch.load(save_path, map_location='cpu'))\n",
    "acc = trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8747bf0e-75c0-4843-8ad7-071bbba5a572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# BaCP Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1377dc7a-a6aa-4c85-8f45-936363972746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Magnitude Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a92e160-d512-47e0-a0c7-ae8c50b6df8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47be290-7787-4221-9b97-b4aeef5734f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if False:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\"\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if False:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b962ef97-9de7-41ca-96f7-9ec3f76bf65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"magnitude_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if False:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if False:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2de2502-7ee6-4605-8863-dd0a372deb79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Movement Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a26f4511-494a-4791-a159-de2542f7a834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99f94d7-44b5-4dbe-8f94-02c47d3955ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f389e8a0-1966-4175-bb47-0e9d5dcb8bf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"movement_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed47df3-7300-495c-aa53-db45eae151d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wanda Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5b9914-fb71-4e73-a33b-dea46e2f3b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_LOW\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3ffae43-49c6-4f13-b214-7683aa9437e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_MID\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c888ec-97f9-4943-9474-4ef2abfcb87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pruning_type = \"wanda_pruning\"\n",
    "target_sparsity = TARGET_SPARSITY_HIGH\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model_task = \"sst2\"\n",
    "trained_model_path = f\"/dbfs/research/{model_name}/{model_task}/{model_name}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArgumentsLLM(\n",
    "    pruning_type,\n",
    "    target_sparsity,\n",
    "    model_name,\n",
    "    model_task,\n",
    "    BATCH_SIZE_DISTILBERT,\n",
    "    trained_model_path,\n",
    "    )\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if True:\n",
    "    bacp_trainer.train()\n",
    "\n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "pruner = bacp_trainer.get_pruner()\n",
    "\n",
    "llm_training_args = LLMTrainingArguments(\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    finetuned_weights=bacp_trainer.cm_save_path,\n",
    "    epochs=5,\n",
    "    prune=False,\n",
    "    pruner=pruner,\n",
    "    finetune=True,\n",
    ")\n",
    "llm_trainer = LLMTrainer(llm_training_args)\n",
    "if True:\n",
    "    llm_trainer.train()\n",
    "\n",
    "acc = llm_trainer.evaluate()\n",
    "print(f\"Accuracy = {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884b20ad-614f-4699-9cbe-fb6246ec88e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /Workspace/\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8922147083857509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "wanda_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
