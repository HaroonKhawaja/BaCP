[34m[1mwandb[0m: Detected [huggingface_hub.inference] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
[TRAINER] Saving model to /dbfs/research/bacp/resnet34/cifar10/resnet34_cifar10_rigl_pruning_0.99_bacp_finetune.pt
[TRAINER] Optimizer type w/ learning rate: (sgd, 0.005)
Traceback (most recent call last):
  File "/Workspace/Users/haroon.khawaja@outlook.com/BaCP/project/test_notebooks/../scripts/bacp_script.py", line 42, in run_training
    bacp_trainer.finetune(run)
  File "/Workspace/Users/haroon.khawaja@outlook.com/BaCP/project/bacp.py", line 352, in finetune
    ft_loss = self._run_finetune_epoch(epoch, desc)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Workspace/Users/haroon.khawaja@outlook.com/BaCP/project/bacp.py", line 422, in _run_finetune_epoch
    _handle_optimizer_and_pruning(self, loss, epoch, step)
  File "/Workspace/Users/haroon.khawaja@outlook.com/BaCP/project/training_utils.py", line 229, in _handle_optimizer_and_pruning
    scaler.step(args.optimizer)
  File "/databricks/python/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 458, in step
    len(optimizer_state["found_inf_per_device"]) > 0
AssertionError: No inf checks were recorded for this optimizer.
