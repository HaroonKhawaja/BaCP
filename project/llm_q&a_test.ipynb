{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a9ae51-88d5-4a44-9724-222fb310ae08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bde51b7-8be0-49a2-8664-fdb723e846b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\haroo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\haroo\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "UsageError: Line magic function `%restart_python` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d864be54-f8b5-4448-b908-2f110b1e7fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = 'cpu'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from bacp import BaCPLearner, BaCPTrainer, BaCPTrainingArgumentsLLM\n",
    "from models import EncoderProjectionNetwork, ClassificationNetwork\n",
    "from unstructured_pruning import MagnitudePrune, MovementPrune, LocalMagnitudePrune, LocalMovementPrune, WandaPrune, PRUNER_DICT, check_model_sparsity\n",
    "from LLM_trainer import LLMTrainer, LLMTrainingArguments\n",
    "from dataset_utils import get_glue_data\n",
    "from logger import Logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset \n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/dbfs/hf_datasets\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from utils import *\n",
    "from constants import *\n",
    "\n",
    "device = get_device()\n",
    "print(f\"{device = }\")\n",
    "BATCH_SIZE_DISTILBERT = 64\n",
    "NUM_WORKERS = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "trainset = dataset['train']\n",
    "valset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "question, answer = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "example = {\n",
    "    'question': question,\n",
    "    'answer': answer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.23"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(example['question'], example['answer'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "inputs = tokenize_fn(example)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "# target is \"nice puppet\"\n",
    "target_start_index = torch.tensor([14])\n",
    "target_end_index = torch.tensor([15])\n",
    "\n",
    "outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "loss = outputs.loss\n",
    "round(loss.item(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this as an example to make squad dataset.\n",
    "# It wont follow the same logic, so dont copy paste it.\n",
    "\n",
    "class GlueDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": example[\"input_ids\"],\n",
    "            \"attention_mask\": example[\"attention_mask\"],\n",
    "            \"labels\": example[\"label\"]\n",
    "        }\n",
    "\n",
    "def get_glue_data(model_name, tokenizer, task_name, batch_size, num_workers=24):\n",
    "    assert task_name in [\"mnli\", \"qqp\", \"sst2\"], f\"Unsupported task: {task_name}\"\n",
    "    dataset = load_dataset(\"glue\", task_name, cache_dir=\"/dbfs/hf_datasets\")\n",
    "    print(f\"[DATALOADERS] {[key for key in dataset]}\")\n",
    "\n",
    "    def tokenize_fn(example):\n",
    "        if task_name == \"mnli\":\n",
    "            return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, padding=\"max_length\")\n",
    "        if task_name == \"qqp\":\n",
    "            return tokenizer(example[\"question1\"], example[\"question2\"], truncation=True, padding=\"max_length\")\n",
    "        if task_name == \"sst2\":\n",
    "            return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\")\n",
    "        \n",
    "    dataset = dataset.map(tokenize_fn, batched=True, batch_size=512, num_proc=1)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    trainset = GlueDataset(dataset[\"train\"])\n",
    "    valset = GlueDataset(dataset[\"validation\"])\n",
    "    testset = GlueDataset(dataset[\"test\"])\n",
    "\n",
    "    loader_args = {\n",
    "        \"batch_size\" : batch_size,\n",
    "        \"num_workers\" : num_workers,\n",
    "        \"pin_memory\" : True,\n",
    "        \"persistent_workers\" : num_workers > 0,\n",
    "        \"drop_last\" : True,\n",
    "    }\n",
    "\n",
    "    trainloader = DataLoader(dataset[\"train\"], shuffle=True, **loader_args)\n",
    "    validationloader = DataLoader(dataset[\"validation\"], **loader_args)\n",
    "    testloader = DataLoader(dataset[\"test\"], **loader_args)\n",
    "  \n",
    "    data = {\n",
    "        \"trainloader\": trainloader,\n",
    "        \"trainset\": trainset,\n",
    "        \"valloader\": validationloader,\n",
    "        \"valset\": valset,\n",
    "        \"testloader\": testloader,\n",
    "        \"testset\": testset\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_squad_data(tokenizer, task_name, batch_size, num_workers=24):\n",
    "    dataset = load_dataset(\"rajpurkar/squad\")\n",
    "    print(f\"[DATALOADERS] {[key for key in dataset]}\")\n",
    "\n",
    "    # Tokenize inputs here - dekhlena how to do it\n",
    "    # There are 2 sets - train and validation. Their size is too big, add some parameter to take a random subset.\n",
    "    # tokenize it properly and return datasets and dataloaders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Script Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Follow the above example ive given to calculate the loss.\n",
    "# Just make a generic training script.\n",
    "# Main functionality should be that it calculates loss, back prop, and optimizer step so it learns, nothing fancy\n",
    "# Ive given the bulk of it.\n",
    "\n",
    "for data in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    inputs = ...\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    answer_start_index = outputs.start_logits.argmax()\n",
    "    answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "    predict_answer_tokens = inputs.input_ids[...]\n",
    "    tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n",
    "\n",
    "    target_start_index = ...\n",
    "    target_end_index = ...\n",
    "    outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8922147083857509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DistilBERT_test_v1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
