Model : resnet50 - Learning Type: bacp_finetune
Configuration:
model_type: cv
model_name: resnet50
model_task: cifar10
num_classes: 10
criterion: CrossEntropyLoss()
embedding_dim: 2048
epochs: 50
pruning_epochs: 50
recovery_epochs: 10
batch_size: 512
learning_rate: 0.0001
learning_type: bacp_finetune
optimizer_type: adamw
use_scheduler: False
prune: False
pruning_type: magnitude_pruning
target_sparsity: 0.95
sparsity_scheduler: linear
delta_t: 41
enable_mixed_precision: True
device: cuda
save_path: /dbfs/research/resnet50/cifar10/resnet50_bacp_finetune_magnitude_pruning_0.92.pt
finetuned_weights: /dbfs/research/resnet50/cifar10/resnet50_magnitude_pruning_0.95_bacp_cm.pt
current_sparsity: 0.9165748434612666

Epoch [1/50]: Avg Loss: 0.7049 | Avg Accuracy: 90.54 | Model Sparsity: 0.9166
Epoch [2/50]: Avg Loss: 0.2672 | Avg Accuracy: 91.36 | Model Sparsity: 0.9166
Epoch [3/50]: Avg Loss: 0.2270 | Avg Accuracy: 91.49 | Model Sparsity: 0.9166
Epoch [4/50]: Avg Loss: 0.2024 | Avg Accuracy: 92.52 | Model Sparsity: 0.9166
Epoch [5/50]: Avg Loss: 0.1851 | Avg Accuracy: 92.38 | Model Sparsity: 0.9166
Epoch [6/50]: Avg Loss: 0.1682 | Avg Accuracy: 92.70 | Model Sparsity: 0.9166
Epoch [7/50]: Avg Loss: 0.1560 | Avg Accuracy: 92.70 | Model Sparsity: 0.9166
Epoch [8/50]: Avg Loss: 0.1438 | Avg Accuracy: 92.70 | Model Sparsity: 0.9166
Epoch [9/50]: Avg Loss: 0.1334 | Avg Accuracy: 92.68 | Model Sparsity: 0.9166
Epoch [10/50]: Avg Loss: 0.1228 | Avg Accuracy: 92.51 | Model Sparsity: 0.9166
Epoch [11/50]: Avg Loss: 0.1166 | Avg Accuracy: 93.07 | Model Sparsity: 0.9166
Epoch [12/50]: Avg Loss: 0.1103 | Avg Accuracy: 92.77 | Model Sparsity: 0.9166
Epoch [13/50]: Avg Loss: 0.1005 | Avg Accuracy: 92.86 | Model Sparsity: 0.9166
Epoch [14/50]: Avg Loss: 0.0952 | Avg Accuracy: 92.75 | Model Sparsity: 0.9166
Epoch [15/50]: Avg Loss: 0.0894 | Avg Accuracy: 92.83 | Model Sparsity: 0.9166
Epoch [16/50]: Avg Loss: 0.0818 | Avg Accuracy: 92.77 | Model Sparsity: 0.9166
Epoch [17/50]: Avg Loss: 0.0763 | Avg Accuracy: 92.93 | Model Sparsity: 0.9166
Epoch [18/50]: Avg Loss: 0.0733 | Avg Accuracy: 92.94 | Model Sparsity: 0.9166
Epoch [19/50]: Avg Loss: 0.0666 | Avg Accuracy: 92.61 | Model Sparsity: 0.9166
Epoch [20/50]: Avg Loss: 0.0665 | Avg Accuracy: 92.86 | Model Sparsity: 0.9166
Epoch [21/50]: Avg Loss: 0.0616 | Avg Accuracy: 92.95 | Model Sparsity: 0.9166
Epoch [22/50]: Avg Loss: 0.0552 | Avg Accuracy: 92.97 | Model Sparsity: 0.9166
Epoch [23/50]: Avg Loss: 0.0563 | Avg Accuracy: 93.19 | Model Sparsity: 0.9166
Epoch [24/50]: Avg Loss: 0.0526 | Avg Accuracy: 92.76 | Model Sparsity: 0.9166
Epoch [25/50]: Avg Loss: 0.0487 | Avg Accuracy: 92.91 | Model Sparsity: 0.9166
Epoch [26/50]: Avg Loss: 0.0463 | Avg Accuracy: 92.83 | Model Sparsity: 0.9166
Epoch [27/50]: Avg Loss: 0.0425 | Avg Accuracy: 92.89 | Model Sparsity: 0.9166
Epoch [28/50]: Avg Loss: 0.0413 | Avg Accuracy: 92.48 | Model Sparsity: 0.9166
Epoch [29/50]: Avg Loss: 0.0408 | Avg Accuracy: 92.69 | Model Sparsity: 0.9166
Epoch [30/50]: Avg Loss: 0.0449 | Avg Accuracy: 92.84 | Model Sparsity: 0.9166
Epoch [31/50]: Avg Loss: 0.0354 | Avg Accuracy: 92.89 | Model Sparsity: 0.9166
Epoch [32/50]: Avg Loss: 0.0372 | Avg Accuracy: 92.26 | Model Sparsity: 0.9166
Epoch [33/50]: Avg Loss: 0.0350 | Avg Accuracy: 92.59 | Model Sparsity: 0.9166
Epoch [34/50]: Avg Loss: 0.0355 | Avg Accuracy: 93.07 | Model Sparsity: 0.9166
Epoch [35/50]: Avg Loss: 0.0340 | Avg Accuracy: 92.47 | Model Sparsity: 0.9166
Epoch [36/50]: Avg Loss: 0.0303 | Avg Accuracy: 92.75 | Model Sparsity: 0.9166
Epoch [37/50]: Avg Loss: 0.0331 | Avg Accuracy: 92.76 | Model Sparsity: 0.9166
Epoch [38/50]: Avg Loss: 0.0294 | Avg Accuracy: 92.55 | Model Sparsity: 0.9166
Epoch [39/50]: Avg Loss: 0.0277 | Avg Accuracy: 92.69 | Model Sparsity: 0.9166
Epoch [40/50]: Avg Loss: 0.0310 | Avg Accuracy: 92.80 | Model Sparsity: 0.9166
Epoch [41/50]: Avg Loss: 0.0293 | Avg Accuracy: 92.80 | Model Sparsity: 0.9166
Epoch [42/50]: Avg Loss: 0.0255 | Avg Accuracy: 92.58 | Model Sparsity: 0.9166
Epoch [43/50]: Avg Loss: 0.0264 | Avg Accuracy: 92.80 | Model Sparsity: 0.9166
