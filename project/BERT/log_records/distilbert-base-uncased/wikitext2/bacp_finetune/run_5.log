Model : distilbert-base-uncased - Learning Type: wikitext2/bacp_finetune
Configuration:
model_type: llm
model_name: distilbert-base-uncased
model_task: wikitext2
num_classes: 30522
embedding_dim: 768
epochs: 50
pruning_epochs: 50
recovery_epochs: 10
batch_size: 64
learning_rate: 0.001
learning_type: bacp_finetune
optimizer_type: adamw
prune: False
pruning_type: magnitude_pruning
target_sparsity: 0.95
sparsity_scheduler: linear
enable_mixed_precision: True
device: cuda
save_path: /dbfs/research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_bacp_finetune.pt
finetuned_weights: /dbfs/research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_pruning.pt

Epoch [1/50]: Avg Loss: 4.4990 | Avg Accuracy: 44.33 | Model Sparsity: 0.9499
Epoch [2/50]: Avg Loss: 3.1250 | Avg Accuracy: 45.70 | Model Sparsity: 0.9499
Epoch [3/50]: Avg Loss: 2.9523 | Avg Accuracy: 46.52 | Model Sparsity: 0.9499
Epoch [4/50]: Avg Loss: 2.8503 | Avg Accuracy: 47.07 | Model Sparsity: 0.9499
Epoch [5/50]: Avg Loss: 2.7653 | Avg Accuracy: 47.00 | Model Sparsity: 0.9499
Epoch [6/50]: Avg Loss: 2.6978 | Avg Accuracy: 47.74 | Model Sparsity: 0.9499
Epoch [7/50]: Avg Loss: 2.6414 | Avg Accuracy: 47.35 | Model Sparsity: 0.9499
Epoch [8/50]: Avg Loss: 2.5898 | Avg Accuracy: 48.15 | Model Sparsity: 0.9499
Epoch [9/50]: Avg Loss: 2.5561 | Avg Accuracy: 47.81 | Model Sparsity: 0.9499
Epoch [10/50]: Avg Loss: 2.5058 | Avg Accuracy: 48.16 | Model Sparsity: 0.9499
Epoch [11/50]: Avg Loss: 2.4794 | Avg Accuracy: 47.88 | Model Sparsity: 0.9499
Epoch [12/50]: Avg Loss: 2.4446 | Avg Accuracy: 48.57 | Model Sparsity: 0.9499
Epoch [13/50]: Avg Loss: 2.4201 | Avg Accuracy: 48.52 | Model Sparsity: 0.9499
Epoch [14/50]: Avg Loss: 2.3853 | Avg Accuracy: 48.34 | Model Sparsity: 0.9499
Epoch [15/50]: Avg Loss: 2.3610 | Avg Accuracy: 48.60 | Model Sparsity: 0.9499
Epoch [16/50]: Avg Loss: 2.3343 | Avg Accuracy: 48.23 | Model Sparsity: 0.9499
Epoch [17/50]: Avg Loss: 2.2970 | Avg Accuracy: 48.26 | Model Sparsity: 0.9499
Epoch [18/50]: Avg Loss: 2.2827 | Avg Accuracy: 47.87 | Model Sparsity: 0.9499
Epoch [19/50]: Avg Loss: 2.2437 | Avg Accuracy: 48.67 | Model Sparsity: 0.9499
Epoch [20/50]: Avg Loss: 2.2333 | Avg Accuracy: 48.37 | Model Sparsity: 0.9499
Epoch [21/50]: Avg Loss: 2.2229 | Avg Accuracy: 49.10 | Model Sparsity: 0.9499
Epoch [22/50]: Avg Loss: 2.1931 | Avg Accuracy: 48.48 | Model Sparsity: 0.9499
Epoch [23/50]: Avg Loss: 2.1761 | Avg Accuracy: 48.80 | Model Sparsity: 0.9499
Epoch [24/50]: Avg Loss: 2.1527 | Avg Accuracy: 48.49 | Model Sparsity: 0.9499
Epoch [25/50]: Avg Loss: 2.1288 | Avg Accuracy: 48.09 | Model Sparsity: 0.9499
Epoch [26/50]: Avg Loss: 2.1260 | Avg Accuracy: 48.71 | Model Sparsity: 0.9499
Epoch [27/50]: Avg Loss: 2.1058 | Avg Accuracy: 48.21 | Model Sparsity: 0.9499
Epoch [28/50]: Avg Loss: 2.0936 | Avg Accuracy: 48.70 | Model Sparsity: 0.9499
Epoch [29/50]: Avg Loss: 2.0728 | Avg Accuracy: 48.56 | Model Sparsity: 0.9499
Epoch [30/50]: Avg Loss: 2.0595 | Avg Accuracy: 48.57 | Model Sparsity: 0.9499
Epoch [31/50]: Avg Loss: 2.0428 | Avg Accuracy: 48.38 | Model Sparsity: 0.9499
Epoch [32/50]: Avg Loss: 2.0344 | Avg Accuracy: 47.88 | Model Sparsity: 0.9499
Epoch [33/50]: Avg Loss: 2.0200 | Avg Accuracy: 48.60 | Model Sparsity: 0.9499
Epoch [34/50]: Avg Loss: 2.0042 | Avg Accuracy: 48.56 | Model Sparsity: 0.9499
Epoch [35/50]: Avg Loss: 1.9939 | Avg Accuracy: 47.75 | Model Sparsity: 0.9499
Epoch [36/50]: Avg Loss: 1.9796 | Avg Accuracy: 48.05 | Model Sparsity: 0.9499
Epoch [37/50]: Avg Loss: 1.9652 | Avg Accuracy: 48.41 | Model Sparsity: 0.9499
Epoch [38/50]: Avg Loss: 1.9488 | Avg Accuracy: 48.35 | Model Sparsity: 0.9499
Epoch [39/50]: Avg Loss: 1.9379 | Avg Accuracy: 48.47 | Model Sparsity: 0.9499
Epoch [40/50]: Avg Loss: 1.9360 | Avg Accuracy: 48.34 | Model Sparsity: 0.9499
Epoch [41/50]: Avg Loss: 1.9254 | Avg Accuracy: 48.50 | Model Sparsity: 0.9499
