Model : distilbert-base-uncased - Learning Type: bacp_pruning/sparsity_0.95
Configuration:
model_type: llm
model_name: distilbert-base-uncased
model_task: sst2
epochs: 10
pruning_epochs: 10
recovery_epochs: 5
batch_size: 64
learning_rate: 1e-05
pruning_type: magnitude_pruning
target_sparsity: 0.95
n_views: 2
temperature: 0.07
base_temperature: 0.07
num_classes: 2
lambda1: 0.25
lambda2: 0.25
lambda3: 0.25
lambda4: 0.25
embedding_dim: 768
save_path: {'current_model': '/dbfs/research/distilbert-base-uncased/sst2/distilbert-base-uncased_magnitude_pruning_0.95_bacp_cm.pt', 'pretrained_model': '/dbfs/research/distilbert-base-uncased/sst2/distilbert-base-uncased_magnitude_pruning_0.95_bacp_pm.pt', 'finetuned_model': '/dbfs/research/distilbert-base-uncased/sst2/distilbert-base-uncased_magnitude_pruning_0.95_bacp_fm.pt'}

Epoch [1/10]: Avg Total Loss: 5.3939 | Avg PrC Loss: 10.2362 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 10.6585 | Avg CE Loss: 0.6809 | Model Sparsity: 0.095
