Model : distilbert-base-uncased - Learning Type: bacp_pruning/magnitude_pruning/0.95
Configuration:
model_name: distilbert-base-uncased
model_task: wikitext2
model_type: llm
num_classes: 30522
batch_size: 64
learning_rate: 0.001
optimizer_type: adamw
epochs: 5
recovery_epochs: 10
patience: 20
pruning_type: magnitude_pruning
target_sparsity: 0.95
sparsity_scheduler: cubic
pruning_epochs: 5
n_views: 2
temperature: 0.07
base_temperature: 0.07
device: cuda
enable_mixed_precision: True
num_workers: 24
train_batches: 71
val_batches: 7
current_model_path: /dbds/research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_cm.pt
pre_trained_model_path: /dbds/research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_pm.pt
finetuned_model_path: /dbds/research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_fm.pt

