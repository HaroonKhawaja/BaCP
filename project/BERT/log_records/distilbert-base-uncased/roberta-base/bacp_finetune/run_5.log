Model : roberta-base - Learning Type: bacp_finetune
Configuration:
model_name: roberta-base
model_task: sst2
num_classes: 2
embedding_dim: 768
batch_size: 64
learning_rate: 2e-05
learning_type: bacp_finetune
epochs: 3
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 2e-05
    maximize: False
    weight_decay: 0.01
)
enable_mixed_precision: True
prune: False
pruning_type: None
pruning_scheduler: linear
target_sparsity: 0.95
pruning_epochs: 3
recovery_epochs: 5
delta_t: 500
finetune: True
save_path: /dbfs/research/roberta-base/sst2/roberta-base_bacp_finetune.pt
initial_sparsity: 0.9499999976452486
device: cuda

Epoch [1/3]: Avg Loss: 0.2621 | Avg Accuracy: 87.14 | Model Sparsity: 0.95
Epoch [2/3]: Avg Loss: 0.2020 | Avg Accuracy: 87.38 | Model Sparsity: 0.95
Epoch [3/3]: Avg Loss: 0.1736 | Avg Accuracy: 88.22 | Model Sparsity: 0.95
