Model : roberta-base - Learning Type: bacp_finetune
Configuration:
model_name: roberta-base
model_task: sst2
num_classes: 2
embedding_dim: 768
batch_size: 64
learning_rate: 2e-05
learning_type: bacp_finetune
epochs: 3
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 2e-05
    maximize: False
    weight_decay: 0.01
)
enable_mixed_precision: True
prune: False
pruning_type: None
pruning_scheduler: linear
target_sparsity: 0.99
pruning_epochs: 3
recovery_epochs: 5
delta_t: 500
finetune: True
save_path: /dbfs/research/roberta-base/sst2/roberta-base_bacp_finetune.pt
initial_sparsity: 0.9900000065933039
device: cuda

Epoch [1/3]: Avg Loss: 0.6818 | Avg Accuracy: 52.64 | Model Sparsity: 0.99
Epoch [2/3]: Avg Loss: 0.5757 | Avg Accuracy: 76.92 | Model Sparsity: 0.99
Epoch [3/3]: Avg Loss: 0.4701 | Avg Accuracy: 78.12 | Model Sparsity: 0.99
