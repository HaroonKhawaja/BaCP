Model : distilbert-base-uncased - Learning Type: bacp_finetune
Configuration:
model_name: distilbert-base-uncased
model_task: sst2
num_classes: 2
embedding_dim: 768
batch_size: 64
learning_rate: 2e-05
learning_type: bacp_finetune
epochs: 3
optimizer: AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 2e-05
    maximize: False
    weight_decay: 0.01
)
enable_mixed_precision: True
prune: False
pruning_type: None
pruning_scheduler: linear
target_sparsity: 0.99
pruning_epochs: 3
recovery_epochs: 5
delta_t: 500
finetune: True
save_path: /dbfs/research/distilbert-base-uncased/sst2/distilbert-base-uncased_bacp_finetune.pt
initial_sparsity: 0.9900000065933039
device: cuda

Epoch [1/3]: Avg Loss: 0.4341 | Avg Accuracy: 83.29 | Model Sparsity: 0.99
Epoch [2/3]: Avg Loss: 0.2557 | Avg Accuracy: 81.97 | Model Sparsity: 0.99
Epoch [3/3]: Avg Loss: 0.2218 | Avg Accuracy: 82.57 | Model Sparsity: 0.99
