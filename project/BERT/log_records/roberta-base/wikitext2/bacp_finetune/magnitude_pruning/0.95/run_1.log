Model : roberta-base - Learning Type: wikitext2/bacp_finetune/magnitude_pruning/0.95
Configuration:
model_type: llm
model_name: roberta-base
model_task: wikitext2
num_classes: 50265
embedding_dim: 768
epochs: 50
pruning_epochs: 50
recovery_epochs: 10
batch_size: 64
learning_rate: 0.001
learning_type: bacp_finetune
optimizer_type: adamw
prune: False
pruning_type: magnitude_pruning
target_sparsity: 0.95
sparsity_scheduler: linear
enable_mixed_precision: True
device: cuda
save_path: /dbfs/research/roberta-base/wikitext2/roberta-base_wikitext2_magnitude_pruning_0.95_bacp_finetune.pt
finetuned_weights: /dbfs/research/roberta-base/wikitext2/roberta-base_wikitext2_magnitude_pruning_0.95_bacp_pruning.pt

Epoch [1/50]: Avg Loss: 3.4071 | Avg Accuracy: 51.45 | Model Sparsity: 0.95
Epoch [2/50]: Avg Loss: 2.7582 | Avg Accuracy: 52.52 | Model Sparsity: 0.95
Epoch [3/50]: Avg Loss: 2.6436 | Avg Accuracy: 53.54 | Model Sparsity: 0.95
Epoch [4/50]: Avg Loss: 2.5403 | Avg Accuracy: 53.69 | Model Sparsity: 0.95
Epoch [5/50]: Avg Loss: 2.4672 | Avg Accuracy: 53.76 | Model Sparsity: 0.95
Epoch [6/50]: Avg Loss: 2.4121 | Avg Accuracy: 54.43 | Model Sparsity: 0.95
Epoch [7/50]: Avg Loss: 2.3627 | Avg Accuracy: 54.17 | Model Sparsity: 0.95
Epoch [8/50]: Avg Loss: 2.3265 | Avg Accuracy: 54.58 | Model Sparsity: 0.95
Epoch [9/50]: Avg Loss: 2.2826 | Avg Accuracy: 55.00 | Model Sparsity: 0.95
Epoch [10/50]: Avg Loss: 2.2486 | Avg Accuracy: 54.81 | Model Sparsity: 0.95
Epoch [11/50]: Avg Loss: 2.2152 | Avg Accuracy: 54.78 | Model Sparsity: 0.95
Epoch [12/50]: Avg Loss: 2.1812 | Avg Accuracy: 55.07 | Model Sparsity: 0.95
Epoch [13/50]: Avg Loss: 2.1568 | Avg Accuracy: 54.84 | Model Sparsity: 0.95
Epoch [14/50]: Avg Loss: 2.1360 | Avg Accuracy: 54.86 | Model Sparsity: 0.95
Epoch [15/50]: Avg Loss: 2.1165 | Avg Accuracy: 54.89 | Model Sparsity: 0.95
Epoch [16/50]: Avg Loss: 2.0888 | Avg Accuracy: 54.79 | Model Sparsity: 0.95
Epoch [17/50]: Avg Loss: 2.0632 | Avg Accuracy: 54.91 | Model Sparsity: 0.95
Epoch [18/50]: Avg Loss: 2.0486 | Avg Accuracy: 55.65 | Model Sparsity: 0.95
Epoch [19/50]: Avg Loss: 2.0348 | Avg Accuracy: 55.24 | Model Sparsity: 0.95
Epoch [20/50]: Avg Loss: 2.0130 | Avg Accuracy: 55.05 | Model Sparsity: 0.95
Epoch [21/50]: Avg Loss: 1.9974 | Avg Accuracy: 54.75 | Model Sparsity: 0.95
Epoch [22/50]: Avg Loss: 1.9835 | Avg Accuracy: 55.83 | Model Sparsity: 0.95
Epoch [23/50]: Avg Loss: 1.9789 | Avg Accuracy: 55.44 | Model Sparsity: 0.95
Epoch [24/50]: Avg Loss: 1.9388 | Avg Accuracy: 55.34 | Model Sparsity: 0.95
Epoch [25/50]: Avg Loss: 1.9327 | Avg Accuracy: 54.97 | Model Sparsity: 0.95
Epoch [26/50]: Avg Loss: 1.9146 | Avg Accuracy: 55.59 | Model Sparsity: 0.95
Epoch [27/50]: Avg Loss: 1.9007 | Avg Accuracy: 55.12 | Model Sparsity: 0.95
Epoch [28/50]: Avg Loss: 1.8908 | Avg Accuracy: 55.43 | Model Sparsity: 0.95
Epoch [29/50]: Avg Loss: 1.8753 | Avg Accuracy: 55.15 | Model Sparsity: 0.95
Epoch [30/50]: Avg Loss: 1.8654 | Avg Accuracy: 54.71 | Model Sparsity: 0.95
Epoch [31/50]: Avg Loss: 1.8507 | Avg Accuracy: 55.57 | Model Sparsity: 0.95
Epoch [32/50]: Avg Loss: 1.8356 | Avg Accuracy: 55.62 | Model Sparsity: 0.95
Epoch [33/50]: Avg Loss: 1.8238 | Avg Accuracy: 55.69 | Model Sparsity: 0.95
Epoch [34/50]: Avg Loss: 1.8097 | Avg Accuracy: 55.47 | Model Sparsity: 0.95
Epoch [35/50]: Avg Loss: 1.8023 | Avg Accuracy: 55.52 | Model Sparsity: 0.95
Epoch [36/50]: Avg Loss: 1.7886 | Avg Accuracy: 55.17 | Model Sparsity: 0.95
Epoch [37/50]: Avg Loss: 1.7806 | Avg Accuracy: 55.15 | Model Sparsity: 0.95
Epoch [38/50]: Avg Loss: 1.7647 | Avg Accuracy: 55.58 | Model Sparsity: 0.95
Epoch [39/50]: Avg Loss: 1.7590 | Avg Accuracy: 55.66 | Model Sparsity: 0.95
Epoch [40/50]: Avg Loss: 1.7477 | Avg Accuracy: 55.60 | Model Sparsity: 0.95
Epoch [41/50]: Avg Loss: 1.7268 | Avg Accuracy: 55.34 | Model Sparsity: 0.95
Epoch [42/50]: Avg Loss: 1.7286 | Avg Accuracy: 55.52 | Model Sparsity: 0.95
