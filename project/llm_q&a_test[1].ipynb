{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a9ae51-88d5-4a44-9724-222fb310ae08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bde51b7-8be0-49a2-8664-fdb723e846b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in c:\\users\\fatim\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%restart_python` not found.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchinfo\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d864be54-f8b5-4448-b908-2f110b1e7fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haroo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = 'cpu'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from bacp import BaCPLearner, BaCPTrainer, BaCPTrainingArgumentsLLM\n",
    "from models import EncoderProjectionNetwork, ClassificationNetwork\n",
    "from unstructured_pruning import MagnitudePrune, MovementPrune, LocalMagnitudePrune, LocalMovementPrune, WandaPrune, PRUNER_DICT, check_model_sparsity\n",
    "from LLM_trainer import LLMTrainer, LLMTrainingArguments\n",
    "from dataset_utils import get_glue_data\n",
    "from logger import Logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset \n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/dbfs/hf_datasets\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "from utils import *\n",
    "from constants import *\n",
    "\n",
    "device = get_device()\n",
    "print(f\"{device = }\")\n",
    "BATCH_SIZE_DISTILBERT = 64\n",
    "NUM_WORKERS = 24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "trainset = dataset['train']\n",
    "valset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "question = \"Who was Jim Henson?\"\n",
    "context = \"Jim Henson was a nice puppet creator.\"\n",
    "answer = \"nice puppet\"\n",
    "start_char = context.index(answer)\n",
    "end_char = start_char + len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: who was jim henson? jim henson was a nice puppet creator.\n",
      "Loss: 6.26\n"
     ]
    }
   ],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example['question'],\n",
    "        example['context'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Prepare example\n",
    "example = {\n",
    "    \"question\": question,\n",
    "    \"context\": context,\n",
    "    \"answer\": answer\n",
    "}\n",
    "\n",
    "inputs = tokenize_fn(example)\n",
    "\n",
    "# Extract and remove offset_mapping (needed for token-char alignment)\n",
    "offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "\n",
    "# Get character-level start and end\n",
    "start_char = context.index(answer)\n",
    "end_char = start_char + len(answer)\n",
    "\n",
    "# Map character positions to token indices\n",
    "start_token = end_token = None\n",
    "for idx, (start, end) in enumerate(offset_mapping):\n",
    "    if start_token is None and start <= start_char < end:\n",
    "        start_token = idx\n",
    "    if start < end_char <= end:\n",
    "        end_token = idx\n",
    "\n",
    "# Fallback in case alignment fails\n",
    "if start_token is None or end_token is None:\n",
    "    raise ValueError(\"Could not find token span for the answer.\")\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "# Decode predicted answer\n",
    "predict_answer_tokens = input_ids[answer_start_index : answer_end_index + 1]\n",
    "print(\"Predicted:\", tokenizer.decode(predict_answer_tokens, skip_special_tokens=True))\n",
    "\n",
    "# Add ground truth span for loss computation\n",
    "inputs[\"start_positions\"] = torch.tensor([start_token])\n",
    "inputs[\"end_positions\"] = torch.tensor([end_token])\n",
    "\n",
    "# Training: forward pass with gold spans\n",
    "outputs = model(**inputs)\n",
    "loss = outputs.loss\n",
    "print(\"Loss:\", round(loss.item(), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, encodings, answer_starts, answer_ends):\n",
    "\n",
    "        self.encodings = {k: (v if isinstance(v, torch.Tensor) else torch.tensor(v, dtype=torch.long))\n",
    "                          for k, v in encodings.items()}\n",
    "        self.answer_starts = torch.tensor(answer_starts, dtype=torch.long)\n",
    "        self.answer_ends = torch.tensor(answer_ends, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx].clone().detach() for k, v in self.encodings.items()}\n",
    "        item['start_positions'] = self.answer_starts[idx].clone().detach()\n",
    "        item['end_positions'] = self.answer_ends[idx].clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.answer_starts.size(0)\n",
    "\n",
    "\n",
    "def get_squad_data(\n",
    "    tokenizer: PreTrainedTokenizerBase,\n",
    "    batch_size: int,\n",
    "    num_workers: int = 0,\n",
    "    max_length: int = 384,\n",
    "    doc_stride: int = 128,\n",
    "    subset_size: int = None\n",
    "):\n",
    "\n",
    "    raw = load_dataset(\"rajpurkar/squad\")\n",
    "\n",
    "    # Optional subset for fast iterations\n",
    "    if subset_size:\n",
    "        for split in ('train', 'validation'):\n",
    "            if split in raw:\n",
    "                raw[split] = raw[split].shuffle(seed=42).select(range(min(subset_size, len(raw[split]))))\n",
    "\n",
    "    def preprocess(examples):\n",
    "        questions = [q.strip() for q in examples['question']]\n",
    "        tokenized = tokenizer(\n",
    "            questions,\n",
    "            examples['context'],\n",
    "            truncation='only_second',\n",
    "            max_length=max_length,\n",
    "            stride=doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "        sample_map = tokenized.pop('overflow_to_sample_mapping')\n",
    "        offsets = tokenized.pop('offset_mapping')\n",
    "\n",
    "        starts, ends = [], []\n",
    "        for i, offset in enumerate(offsets):\n",
    "            input_ids = tokenized['input_ids'][i]\n",
    "            cls_idx = input_ids.index(tokenizer.cls_token_id)\n",
    "            seq_ids = tokenized.sequence_ids(i)\n",
    "            sample_idx = sample_map[i]\n",
    "            answer = examples['answers'][sample_idx]\n",
    "            if len(answer['answer_start']) == 0:\n",
    "                starts.append(cls_idx)\n",
    "                ends.append(cls_idx)\n",
    "            else:\n",
    "                start_char = answer['answer_start'][0]\n",
    "                end_char = start_char + len(answer['text'][0])\n",
    "                # find window token indices\n",
    "                token_start, token_end = 0, len(input_ids) - 1\n",
    "                while seq_ids[token_start] != 1:\n",
    "                    token_start += 1\n",
    "                while seq_ids[token_end] != 1:\n",
    "                    token_end -= 1\n",
    "                # answer out of window\n",
    "                if not (offset[token_start][0] <= start_char and offset[token_end][1] >= end_char):\n",
    "                    starts.append(cls_idx)\n",
    "                    ends.append(cls_idx)\n",
    "                else:\n",
    "                    # move to exact token boundaries\n",
    "                    while token_start < len(offset) and offset[token_start][0] <= start_char:\n",
    "                        token_start += 1\n",
    "                    starts.append(token_start - 1)\n",
    "                    while offset[token_end][1] >= end_char:\n",
    "                        token_end -= 1\n",
    "                    ends.append(token_end + 1)\n",
    "        tokenized['start_positions'] = starts\n",
    "        tokenized['end_positions'] = ends\n",
    "        return tokenized\n",
    "\n",
    "    datasets = {}\n",
    "    dataloaders = {}\n",
    "    for split in ('train', 'validation'):\n",
    "        if split in raw:\n",
    "            proc = raw[split].map(\n",
    "                preprocess,\n",
    "                batched=True,\n",
    "                remove_columns=raw[split].column_names\n",
    "            )\n",
    "            proc.set_format(type='torch')\n",
    "            encodings = {\n",
    "                'input_ids': proc['input_ids'],\n",
    "                'attention_mask': proc['attention_mask']\n",
    "            }\n",
    "            ds = SquadDataset(encodings, proc['start_positions'], proc['end_positions'])\n",
    "            loader = DataLoader(\n",
    "                ds,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=(split=='train'),\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=(num_workers>0),\n",
    "                drop_last=(split=='train')\n",
    "            )\n",
    "            datasets[f'{split}_dataset'] = ds\n",
    "            dataloaders[f'{split}_loader'] = loader\n",
    "    # No test split available\n",
    "    dataloaders['test_loader'] = None\n",
    "    datasets['test_dataset'] = None\n",
    "\n",
    "    return {**datasets, **dataloaders}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch from train_loader:\n",
      "  input_ids shape:      torch.Size([4, 384])\n",
      "  attention_mask shape: torch.Size([4, 384])\n",
      "  start_positions shape: torch.Size([4])\n",
      "  end_positions shape:   torch.Size([4])\n",
      "\n",
      "Batch from validation_loader:\n",
      "  input_ids shape:      torch.Size([4, 384])\n",
      "  attention_mask shape: torch.Size([4, 384])\n",
      "  start_positions shape: torch.Size([4])\n",
      "  end_positions shape:   torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haroo\\AppData\\Local\\Temp\\ipykernel_952\\3006852678.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.answer_starts = torch.tensor(answer_starts, dtype=torch.long)\n",
      "C:\\Users\\haroo\\AppData\\Local\\Temp\\ipykernel_952\\3006852678.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.answer_ends = torch.tensor(answer_ends, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer (you already have this)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Test the function\n",
    "squad_data = get_squad_data(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,      # Small batch size for testing\n",
    "    num_workers=0,     # Simpler for initial testing\n",
    "    subset_size=100    # Use a small subset for faster testing\n",
    ")\n",
    "\n",
    "# Check the train loader\n",
    "train_loader = squad_data[\"train_loader\"]\n",
    "if train_loader is not None:\n",
    "    for batch in train_loader:\n",
    "        print(\"Batch from train_loader:\")\n",
    "        print(\"  input_ids shape:     \", batch['input_ids'].shape)\n",
    "        print(\"  attention_mask shape:\", batch['attention_mask'].shape)\n",
    "        print(\"  start_positions shape:\", batch['start_positions'].shape)\n",
    "        print(\"  end_positions shape:  \", batch['end_positions'].shape)\n",
    "        break\n",
    "else:\n",
    "    print(\"Train loader is None.\")\n",
    "\n",
    "# Check the validation loader\n",
    "val_loader = squad_data[\"validation_loader\"]\n",
    "if val_loader is not None:\n",
    "    for batch in val_loader:\n",
    "        print(\"\\nBatch from validation_loader:\")\n",
    "        print(\"  input_ids shape:     \", batch['input_ids'].shape)\n",
    "        print(\"  attention_mask shape:\", batch['attention_mask'].shape)\n",
    "        print(\"  start_positions shape:\", batch['start_positions'].shape)\n",
    "        print(\"  end_positions shape:  \", batch['end_positions'].shape)\n",
    "        break\n",
    "else:\n",
    "    print(\"Validation loader is None.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use this as an example to make squad dataset.\n",
    "# # It wont follow the same logic, so dont copy paste it.\n",
    "\n",
    "# class GlueDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         example = self.data[idx]\n",
    "#         return {\n",
    "#             \"input_ids\": example[\"input_ids\"],\n",
    "#             \"attention_mask\": example[\"attention_mask\"],\n",
    "#             \"labels\": example[\"label\"]\n",
    "#         }\n",
    "\n",
    "# def get_glue_data(model_name, tokenizer, task_name, batch_size, num_workers=24):\n",
    "#     assert task_name in [\"mnli\", \"qqp\", \"sst2\"], f\"Unsupported task: {task_name}\"\n",
    "#     dataset = load_dataset(\"glue\", task_name, cache_dir=\"/dbfs/hf_datasets\")\n",
    "#     print(f\"[DATALOADERS] {[key for key in dataset]}\")\n",
    "\n",
    "#     def tokenize_fn(example):\n",
    "#         if task_name == \"mnli\":\n",
    "#             return tokenizer(example[\"premise\"], example[\"hypothesis\"], truncation=True, padding=\"max_length\")\n",
    "#         if task_name == \"qqp\":\n",
    "#             return tokenizer(example[\"question1\"], example[\"question2\"], truncation=True, padding=\"max_length\")\n",
    "#         if task_name == \"sst2\":\n",
    "#             return tokenizer(example[\"sentence\"], truncation=True, padding=\"max_length\")\n",
    "        \n",
    "#     dataset = dataset.map(tokenize_fn, batched=True, batch_size=512, num_proc=1)\n",
    "#     dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "#     trainset = GlueDataset(dataset[\"train\"])\n",
    "#     valset = GlueDataset(dataset[\"validation\"])\n",
    "#     testset = GlueDataset(dataset[\"test\"])\n",
    "\n",
    "#     loader_args = {\n",
    "#         \"batch_size\" : batch_size,\n",
    "#         \"num_workers\" : num_workers,\n",
    "#         \"pin_memory\" : True,\n",
    "#         \"persistent_workers\" : num_workers > 0,\n",
    "#         \"drop_last\" : True,\n",
    "#     }\n",
    "\n",
    "#     trainloader = DataLoader(dataset[\"train\"], shuffle=True, **loader_args)\n",
    "#     validationloader = DataLoader(dataset[\"validation\"], **loader_args)\n",
    "#     testloader = DataLoader(dataset[\"test\"], **loader_args)\n",
    "  \n",
    "#     data = {\n",
    "#         \"trainloader\": trainloader,\n",
    "#         \"trainset\": trainset,\n",
    "#         \"valloader\": validationloader,\n",
    "#         \"valset\": valset,\n",
    "#         \"testloader\": testloader,\n",
    "#         \"testset\": testset\n",
    "#     }\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_squad_data(tokenizer, task_name, batch_size, num_workers=24):\n",
    "#     dataset = load_dataset(\"rajpurkar/squad\")\n",
    "#     print(f\"[DATALOADERS] {[key for key in dataset]}\")\n",
    "\n",
    "#     # Tokenize inputs here - dekhlena how to do it\n",
    "#     # There are 2 sets - train and validation. Their size is too big, add some parameter to take a random subset.\n",
    "#     # tokenize it properly and return datasets and dataloaders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Script Here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DistilBertForQuestionAnswering, get_linear_schedule_with_warmup\n",
    "def train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_loader,\n",
    "    validation_loader=None,\n",
    "    output_dir=\"./squad_model\",\n",
    "    num_epochs=3,\n",
    "    lr=5e-5,\n",
    "    device=None\n",
    "):\n",
    "    \n",
    "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Epoch loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        batchloader = tqdm(train_loader, desc=f\"Epoch {epoch} Training\", leave=False)\n",
    "        for step, batch in enumerate(batchloader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                start_positions=batch['start_positions'],\n",
    "                end_positions=batch['end_positions']\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            running_loss += loss.item() / (step + 1)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batchloader.set_postfix(Running_Loss=f\"{running_loss:.5f}\")\n",
    "\n",
    "        avg_train_loss = running_loss\n",
    "        print(f\"Epoch {epoch} training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        if validation_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for batch in tqdm(validation_loader, desc=f\"Epoch {epoch} Validation\", leave=False):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        start_positions=batch['start_positions'],\n",
    "                        end_positions=batch['end_positions']\n",
    "                    )\n",
    "                val_loss += outputs.loss.item()\n",
    "            avg_val_loss = val_loss / len(validation_loader)\n",
    "            print(f\"Epoch {epoch} validation loss: {avg_val_loss:.4f}\")\n",
    "            model.train()\n",
    "\n",
    "        # Save checkpoint\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        ckpt_dir = os.path.join(output_dir, f\"checkpoint-epoch{epoch}\")\n",
    "        model.save_pretrained(ckpt_dir)\n",
    "        tokenizer.save_pretrained(ckpt_dir)\n",
    "        print(f\"Saved checkpoint for epoch {epoch} at {ckpt_dir}\")\n",
    "\n",
    "    # Final save\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Training complete. Final model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 20.3418\n",
      "Saved checkpoint for epoch 1 at ./squad_model\\checkpoint-epoch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss: 13.6800\n",
      "Saved checkpoint for epoch 2 at ./squad_model\\checkpoint-epoch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss: 10.0323\n",
      "Saved checkpoint for epoch 3 at ./squad_model\\checkpoint-epoch3\n",
      "Training complete. Final model saved to ./squad_model\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_loader,\n",
    "    validation_loader=None,\n",
    "    output_dir=\"./squad_model\",\n",
    "    num_epochs=3,\n",
    "    lr=5e-5,\n",
    "    device=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: jim henson\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was Jim Henson?\"\n",
    "context = \"Jim Henson was a nice puppet creator.\"\n",
    "answer = \"nice puppet\"\n",
    "start_char = context.index(answer)\n",
    "end_char = start_char + len(answer)\n",
    "\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example['question'],\n",
    "        example['context'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Prepare example\n",
    "example = {\n",
    "    \"question\": question,\n",
    "    \"context\": context,\n",
    "    \"answer\": answer\n",
    "}\n",
    "\n",
    "inputs = tokenize_fn(example)\n",
    "\n",
    "# Extract and remove offset_mapping (needed for token-char alignment)\n",
    "offset_mapping = inputs.pop(\"offset_mapping\")[0]\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "\n",
    "# Get character-level start and end\n",
    "start_char = context.index(answer)\n",
    "end_char = start_char + len(answer)\n",
    "\n",
    "# Map character positions to token indices\n",
    "start_token = end_token = None\n",
    "for idx, (start, end) in enumerate(offset_mapping):\n",
    "    if start_token is None and start <= start_char < end:\n",
    "        start_token = idx\n",
    "    if start < end_char <= end:\n",
    "        end_token = idx\n",
    "\n",
    "# Fallback in case alignment fails\n",
    "if start_token is None or end_token is None:\n",
    "    raise ValueError(\"Could not find token span for the answer.\")\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()\n",
    "\n",
    "# Decode predicted answer\n",
    "predict_answer_tokens = input_ids[answer_start_index : answer_end_index + 1]\n",
    "print(\"Predicted:\", tokenizer.decode(predict_answer_tokens, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8922147083857509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DistilBERT_test_v1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
