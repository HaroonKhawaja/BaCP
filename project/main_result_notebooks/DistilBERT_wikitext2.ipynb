{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a9ae51-88d5-4a44-9724-222fb310ae08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d864be54-f8b5-4448-b908-2f110b1e7fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from constants import (\n",
    "    TARGET_SPARSITY_LOW, TARGET_SPARSITY_MID, TARGET_SPARSITY_HIGH,\n",
    "    BATCH_SIZE_CNN, BATCH_SIZE_VIT, BATCH_SIZE_LLM,\n",
    "    EPOCHS_SMALL_MODEL, EPOCHS_LARGE_MODEL, EPOCHS_VIT\n",
    ")\n",
    "from utils import get_device, get_num_workers, load_weights, print_statistics\n",
    "from unstructured_pruning import check_model_sparsity, check_sparsity_distribution\n",
    "from trainer import TrainingArguments, Trainer\n",
    "from bacp import BaCPTrainingArguments, BaCPTrainer\n",
    "\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "disable_progress_bar()\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/dbfs/hf_datasets\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using 288 workers\n"
     ]
    }
   ],
   "source": [
    "DEVICE = get_device()\n",
    "NUM_WORKERS = get_num_workers()\n",
    "print(\"Using device:\", DEVICE)\n",
    "print(\"Using\", NUM_WORKERS, \"workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b9a306d-0095-4803-9684-d7530d217884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "MODEL_TASK = \"wikitext2\"\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22ccfbfc-6204-4ca3-b707-7ad645dcce8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aeec2d7-7476-4e02-959a-ee25673c4620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 5e-05)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] Linear scheduler initialized with warmup steps: 355 and total steps: 3550\n",
      "[TRAINER] Pruning not initialized\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     64.00%\n",
      "  Perplexity:   5.743\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.0000 (0.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        baseline\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        5e-05\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               50\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-5),\n",
    "    scheduler_type='linear_with_warmup',\n",
    "    epochs=50,\n",
    "    learning_type=\"baseline\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args=training_args)\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be69d1d-373f-4b12-a48d-9ab5140e1728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pruning Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d58a036-4334-44c0-823e-031ae7e08669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Magnitude Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d98519d4-561a-4ecd-9fc3-9c5394efc5e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Target sparsity: 0.95\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     47.98%\n",
      "  Perplexity:   27.106\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9500 (95.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         magnitude_pruning\n",
      "  Target Sparsity:      0.95\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if False:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "defe99f3-fbdb-4502-b1a1-d11ce121da08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Target sparsity: 0.97\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.97_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.97_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     45.93%\n",
      "  Perplexity:   30.643\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9700 (97.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         magnitude_pruning\n",
      "  Target Sparsity:      0.97\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97ba3b50-f1da-447b-9cb2-909914a423a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Target sparsity: 0.99\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.99_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.99_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     41.23%\n",
      "  Perplexity:   45.285\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9900 (99.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         magnitude_pruning\n",
      "  Target Sparsity:      0.99\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d1270a-6f28-4e85-8416-c473016644cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SNIP-it Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab38d80e-18d7-4212-a829-d6d300d611cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: snip_pruning\n",
      "[TRAINER] Target sparsity: 0.95\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.95_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.95_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     45.38%\n",
      "  Perplexity:   34.360\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9500 (95.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         snip_pruning\n",
      "  Target Sparsity:      0.95\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d511df92-bbae-4f32-88be-368dab9b0d5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: snip_pruning\n",
      "[TRAINER] Target sparsity: 0.97\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.97_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.97_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     42.39%\n",
      "  Perplexity:   39.963\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9700 (97.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         snip_pruning\n",
      "  Target Sparsity:      0.97\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44235e91-261e-4573-ba7a-e422171d8e37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: snip_pruning\n",
      "[TRAINER] Target sparsity: 0.99\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.99_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_snip_pruning_0.99_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     37.65%\n",
      "  Perplexity:   64.528\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9900 (99.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         snip_pruning\n",
      "  Target Sparsity:      0.99\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b8b53cd-9626-4b42-8655-6cfe9cf07194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WandA Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4ffcd4-02dd-41d9-9870-36e12b20926f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      ".weight\n",
      "model.weight\n",
      "model.activation.weight\n",
      "model.distilbert.weight\n",
      "model.distilbert.embeddings.weight\n",
      "model.distilbert.embeddings.word_embeddings.weight\n",
      "model.distilbert.embeddings.position_embeddings.weight\n",
      "model.distilbert.embeddings.LayerNorm.weight\n",
      "model.distilbert.embeddings.dropout.weight\n",
      "model.distilbert.transformer.weight\n",
      "model.distilbert.transformer.layer.weight\n",
      "model.distilbert.transformer.layer.0.weight\n",
      "model.distilbert.transformer.layer.0.attention.weight\n",
      "model.distilbert.transformer.layer.0.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.0.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.0.ffn.weight\n",
      "model.distilbert.transformer.layer.0.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.0.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.weight\n",
      "model.distilbert.transformer.layer.1.attention.weight\n",
      "model.distilbert.transformer.layer.1.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.1.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.ffn.weight\n",
      "model.distilbert.transformer.layer.1.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.1.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.weight\n",
      "model.distilbert.transformer.layer.2.attention.weight\n",
      "model.distilbert.transformer.layer.2.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.2.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.ffn.weight\n",
      "model.distilbert.transformer.layer.2.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.2.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.weight\n",
      "model.distilbert.transformer.layer.3.attention.weight\n",
      "model.distilbert.transformer.layer.3.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.3.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.ffn.weight\n",
      "model.distilbert.transformer.layer.3.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.3.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.weight\n",
      "model.distilbert.transformer.layer.4.attention.weight\n",
      "model.distilbert.transformer.layer.4.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.4.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.ffn.weight\n",
      "model.distilbert.transformer.layer.4.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.4.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.weight\n",
      "model.distilbert.transformer.layer.5.attention.weight\n",
      "model.distilbert.transformer.layer.5.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.5.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.ffn.weight\n",
      "model.distilbert.transformer.layer.5.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.5.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "model.vocab_transform.weight\n",
      "model.vocab_layer_norm.weight\n",
      "model.vocab_projector.weight\n",
      "model.mlm_loss_fct.weight\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: wanda_pruning\n",
      "[TRAINER] Target sparsity: 0.95\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.95_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.95_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     45.70%\n",
      "  Perplexity:   34.683\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9500 (95.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         wanda_pruning\n",
      "  Target Sparsity:      0.95\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79cfef5e-ce9c-40e6-873f-05e90d5c2ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      ".weight\n",
      "model.weight\n",
      "model.activation.weight\n",
      "model.distilbert.weight\n",
      "model.distilbert.embeddings.weight\n",
      "model.distilbert.embeddings.word_embeddings.weight\n",
      "model.distilbert.embeddings.position_embeddings.weight\n",
      "model.distilbert.embeddings.LayerNorm.weight\n",
      "model.distilbert.embeddings.dropout.weight\n",
      "model.distilbert.transformer.weight\n",
      "model.distilbert.transformer.layer.weight\n",
      "model.distilbert.transformer.layer.0.weight\n",
      "model.distilbert.transformer.layer.0.attention.weight\n",
      "model.distilbert.transformer.layer.0.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.0.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.0.ffn.weight\n",
      "model.distilbert.transformer.layer.0.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.0.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.weight\n",
      "model.distilbert.transformer.layer.1.attention.weight\n",
      "model.distilbert.transformer.layer.1.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.1.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.ffn.weight\n",
      "model.distilbert.transformer.layer.1.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.1.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.weight\n",
      "model.distilbert.transformer.layer.2.attention.weight\n",
      "model.distilbert.transformer.layer.2.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.2.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.ffn.weight\n",
      "model.distilbert.transformer.layer.2.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.2.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.weight\n",
      "model.distilbert.transformer.layer.3.attention.weight\n",
      "model.distilbert.transformer.layer.3.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.3.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.ffn.weight\n",
      "model.distilbert.transformer.layer.3.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.3.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.weight\n",
      "model.distilbert.transformer.layer.4.attention.weight\n",
      "model.distilbert.transformer.layer.4.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.4.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.ffn.weight\n",
      "model.distilbert.transformer.layer.4.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.4.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.weight\n",
      "model.distilbert.transformer.layer.5.attention.weight\n",
      "model.distilbert.transformer.layer.5.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.5.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.ffn.weight\n",
      "model.distilbert.transformer.layer.5.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.5.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "model.vocab_transform.weight\n",
      "model.vocab_layer_norm.weight\n",
      "model.vocab_projector.weight\n",
      "model.mlm_loss_fct.weight\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: wanda_pruning\n",
      "[TRAINER] Target sparsity: 0.97\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.97_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.97_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     43.57%\n",
      "  Perplexity:   38.139\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9700 (97.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         wanda_pruning\n",
      "  Target Sparsity:      0.97\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a21b89-e8aa-4377-a402-7e1cd7f66155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_baseline.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.0005)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      ".weight\n",
      "model.weight\n",
      "model.activation.weight\n",
      "model.distilbert.weight\n",
      "model.distilbert.embeddings.weight\n",
      "model.distilbert.embeddings.word_embeddings.weight\n",
      "model.distilbert.embeddings.position_embeddings.weight\n",
      "model.distilbert.embeddings.LayerNorm.weight\n",
      "model.distilbert.embeddings.dropout.weight\n",
      "model.distilbert.transformer.weight\n",
      "model.distilbert.transformer.layer.weight\n",
      "model.distilbert.transformer.layer.0.weight\n",
      "model.distilbert.transformer.layer.0.attention.weight\n",
      "model.distilbert.transformer.layer.0.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.0.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.0.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.0.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.0.ffn.weight\n",
      "model.distilbert.transformer.layer.0.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.0.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.0.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.0.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.weight\n",
      "model.distilbert.transformer.layer.1.attention.weight\n",
      "model.distilbert.transformer.layer.1.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.1.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.1.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.1.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.1.ffn.weight\n",
      "model.distilbert.transformer.layer.1.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.1.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.1.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.1.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.weight\n",
      "model.distilbert.transformer.layer.2.attention.weight\n",
      "model.distilbert.transformer.layer.2.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.2.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.2.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.2.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.2.ffn.weight\n",
      "model.distilbert.transformer.layer.2.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.2.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.2.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.2.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.weight\n",
      "model.distilbert.transformer.layer.3.attention.weight\n",
      "model.distilbert.transformer.layer.3.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.3.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.3.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.3.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.3.ffn.weight\n",
      "model.distilbert.transformer.layer.3.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.3.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.3.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.3.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.weight\n",
      "model.distilbert.transformer.layer.4.attention.weight\n",
      "model.distilbert.transformer.layer.4.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.4.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.4.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.4.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.4.ffn.weight\n",
      "model.distilbert.transformer.layer.4.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.4.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.4.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.4.output_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.weight\n",
      "model.distilbert.transformer.layer.5.attention.weight\n",
      "model.distilbert.transformer.layer.5.attention.dropout.weight\n",
      "model.distilbert.transformer.layer.5.attention.q_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.k_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.v_lin.weight\n",
      "model.distilbert.transformer.layer.5.attention.out_lin.weight\n",
      "model.distilbert.transformer.layer.5.sa_layer_norm.weight\n",
      "model.distilbert.transformer.layer.5.ffn.weight\n",
      "model.distilbert.transformer.layer.5.ffn.dropout.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin1.weight\n",
      "model.distilbert.transformer.layer.5.ffn.lin2.weight\n",
      "model.distilbert.transformer.layer.5.ffn.activation.weight\n",
      "model.distilbert.transformer.layer.5.output_layer_norm.weight\n",
      "model.vocab_transform.weight\n",
      "model.vocab_layer_norm.weight\n",
      "model.vocab_projector.weight\n",
      "model.mlm_loss_fct.weight\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: wanda_pruning\n",
      "[TRAINER] Target sparsity: 0.99\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.99_pruning.pt\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_wanda_pruning_0.99_pruning.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     40.85%\n",
      "  Perplexity:   48.597\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9900 (99.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        pruning\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.0005\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               5\n",
      "\n",
      "Pruning Configuration:\n",
      "------------------------------\n",
      "  Pruning Type:         wanda_pruning\n",
      "  Target Sparsity:      0.99\n",
      "  Sparsity Scheduler:   cubic\n",
      "  Recovery Epochs:      10\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Initializing finetuned weights path\n",
    "finetuned_weights = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "training_args = TrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,\n",
    "    optimizer_type_and_lr=('adamw', 5e-4),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=finetuned_weights,\n",
    "    learning_type=\"pruning\",\n",
    "    db=False,\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8747bf0e-75c0-4843-8ad7-071bbba5a572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## BaCP Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a152fcf0-b6c5-4f04-8e5b-26ca6ee34ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Magnitude Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9a7b8b-4a1a-4f2c-9202-18f79790bcec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Initialized BaCP models\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.001)\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Target sparsity: 0.95\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_pruning.pt\n",
      "[LOGGER] Log file created at location: ./log_records/distilbert-base-uncased/wikitext2/bacp_pruning/magnitude_pruning/0.95/run_4.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.464.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: Avg Total Loss: 7.6227 | Avg PrC Loss: 2.6096 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.4856 | Avg CE Loss: 2.5275 | Model Sparsity: 0.4636\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 6.6930 | Avg PrC Loss: 2.2470 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0331 | Avg CE Loss: 2.4129 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 6.5663 | Avg PrC Loss: 2.2379 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0104 | Avg CE Loss: 2.3179 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 6.4671 | Avg PrC Loss: 2.2390 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0061 | Avg CE Loss: 2.2221 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 6.3978 | Avg PrC Loss: 2.2537 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0184 | Avg CE Loss: 2.1258 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 6.3208 | Avg PrC Loss: 2.2600 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0232 | Avg CE Loss: 2.0376 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 6.2463 | Avg PrC Loss: 2.2672 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0282 | Avg CE Loss: 1.9510 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 6.1967 | Avg PrC Loss: 2.2791 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0397 | Avg CE Loss: 1.8780 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 6.1418 | Avg PrC Loss: 2.2837 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0428 | Avg CE Loss: 1.8153 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 6.0877 | Avg PrC Loss: 2.2815 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0392 | Avg CE Loss: 1.7670 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 6.0504 | Avg PrC Loss: 2.2813 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0373 | Avg CE Loss: 1.7317 | Model Sparsity: 0.4636\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.745.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: Avg Total Loss: 9.6940 | Avg PrC Loss: 2.9796 | Avg SnC Loss: 1.9367 | Avg FiC Loss: 2.9129 | Avg CE Loss: 1.8648 | Model Sparsity: 0.7448\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 8.4258 | Avg PrC Loss: 2.5940 | Avg SnC Loss: 1.6468 | Avg FiC Loss: 2.4622 | Avg CE Loss: 1.7228 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 8.2697 | Avg PrC Loss: 2.5431 | Avg SnC Loss: 1.6452 | Avg FiC Loss: 2.3951 | Avg CE Loss: 1.6863 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 8.1903 | Avg PrC Loss: 2.5228 | Avg SnC Loss: 1.6447 | Avg FiC Loss: 2.3638 | Avg CE Loss: 1.6590 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 8.1185 | Avg PrC Loss: 2.5039 | Avg SnC Loss: 1.6416 | Avg FiC Loss: 2.3377 | Avg CE Loss: 1.6353 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 8.0666 | Avg PrC Loss: 2.4910 | Avg SnC Loss: 1.6392 | Avg FiC Loss: 2.3188 | Avg CE Loss: 1.6175 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 8.0210 | Avg PrC Loss: 2.4823 | Avg SnC Loss: 1.6371 | Avg FiC Loss: 2.3055 | Avg CE Loss: 1.5961 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 7.9895 | Avg PrC Loss: 2.4765 | Avg SnC Loss: 1.6402 | Avg FiC Loss: 2.2969 | Avg CE Loss: 1.5760 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 7.9619 | Avg PrC Loss: 2.4712 | Avg SnC Loss: 1.6372 | Avg FiC Loss: 2.2885 | Avg CE Loss: 1.5650 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 7.9268 | Avg PrC Loss: 2.4643 | Avg SnC Loss: 1.6323 | Avg FiC Loss: 2.2793 | Avg CE Loss: 1.5509 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 7.8955 | Avg PrC Loss: 2.4564 | Avg SnC Loss: 1.6299 | Avg FiC Loss: 2.2700 | Avg CE Loss: 1.5392 | Model Sparsity: 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.889.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: Avg Total Loss: 11.6658 | Avg PrC Loss: 3.0429 | Avg SnC Loss: 3.8011 | Avg FiC Loss: 2.9977 | Avg CE Loss: 1.8241 | Model Sparsity: 0.8892\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 10.3376 | Avg PrC Loss: 2.7680 | Avg SnC Loss: 3.2002 | Avg FiC Loss: 2.6811 | Avg CE Loss: 1.6882 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 10.1564 | Avg PrC Loss: 2.7234 | Avg SnC Loss: 3.1591 | Avg FiC Loss: 2.6244 | Avg CE Loss: 1.6495 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 10.0532 | Avg PrC Loss: 2.6984 | Avg SnC Loss: 3.1386 | Avg FiC Loss: 2.5912 | Avg CE Loss: 1.6250 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 9.9922 | Avg PrC Loss: 2.6845 | Avg SnC Loss: 3.1322 | Avg FiC Loss: 2.5720 | Avg CE Loss: 1.6034 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 9.9364 | Avg PrC Loss: 2.6706 | Avg SnC Loss: 3.1243 | Avg FiC Loss: 2.5535 | Avg CE Loss: 1.5880 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 9.8877 | Avg PrC Loss: 2.6588 | Avg SnC Loss: 3.1145 | Avg FiC Loss: 2.5383 | Avg CE Loss: 1.5762 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 9.8521 | Avg PrC Loss: 2.6521 | Avg SnC Loss: 3.1073 | Avg FiC Loss: 2.5281 | Avg CE Loss: 1.5647 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 9.8123 | Avg PrC Loss: 2.6404 | Avg SnC Loss: 3.1022 | Avg FiC Loss: 2.5157 | Avg CE Loss: 1.5540 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 9.8017 | Avg PrC Loss: 2.6414 | Avg SnC Loss: 3.1042 | Avg FiC Loss: 2.5131 | Avg CE Loss: 1.5430 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 9.7609 | Avg PrC Loss: 2.6313 | Avg SnC Loss: 3.0928 | Avg FiC Loss: 2.5018 | Avg CE Loss: 1.5350 | Model Sparsity: 0.8892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.942.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: Avg Total Loss: 13.1588 | Avg PrC Loss: 3.0512 | Avg SnC Loss: 5.3616 | Avg FiC Loss: 2.9993 | Avg CE Loss: 1.7467 | Model Sparsity: 0.9424\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 12.0958 | Avg PrC Loss: 2.8494 | Avg SnC Loss: 4.8206 | Avg FiC Loss: 2.7748 | Avg CE Loss: 1.6509 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 11.9648 | Avg PrC Loss: 2.8189 | Avg SnC Loss: 4.7855 | Avg FiC Loss: 2.7370 | Avg CE Loss: 1.6233 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 11.8718 | Avg PrC Loss: 2.7972 | Avg SnC Loss: 4.7499 | Avg FiC Loss: 2.7122 | Avg CE Loss: 1.6125 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 11.8089 | Avg PrC Loss: 2.7847 | Avg SnC Loss: 4.7321 | Avg FiC Loss: 2.6959 | Avg CE Loss: 1.5962 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 11.7529 | Avg PrC Loss: 2.7726 | Avg SnC Loss: 4.7146 | Avg FiC Loss: 2.6810 | Avg CE Loss: 1.5847 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 11.7119 | Avg PrC Loss: 2.7649 | Avg SnC Loss: 4.7017 | Avg FiC Loss: 2.6714 | Avg CE Loss: 1.5738 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 11.6647 | Avg PrC Loss: 2.7541 | Avg SnC Loss: 4.6843 | Avg FiC Loss: 2.6588 | Avg CE Loss: 1.5675 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 11.6485 | Avg PrC Loss: 2.7518 | Avg SnC Loss: 4.6839 | Avg FiC Loss: 2.6545 | Avg CE Loss: 1.5583 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 11.6340 | Avg PrC Loss: 2.7508 | Avg SnC Loss: 4.6845 | Avg FiC Loss: 2.6507 | Avg CE Loss: 1.5481 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 11.5911 | Avg PrC Loss: 2.7435 | Avg SnC Loss: 4.6637 | Avg FiC Loss: 2.6417 | Avg CE Loss: 1.5422 | Model Sparsity: 0.9424\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [5/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.950.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5]: Avg Total Loss: 13.3861 | Avg PrC Loss: 2.7970 | Avg SnC Loss: 6.3221 | Avg FiC Loss: 2.7087 | Avg CE Loss: 1.5583 | Model Sparsity: 0.95\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 13.2747 | Avg PrC Loss: 2.7793 | Avg SnC Loss: 6.2676 | Avg FiC Loss: 2.6864 | Avg CE Loss: 1.5415 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 13.2405 | Avg PrC Loss: 2.7739 | Avg SnC Loss: 6.2537 | Avg FiC Loss: 2.6797 | Avg CE Loss: 1.5332 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 13.2162 | Avg PrC Loss: 2.7712 | Avg SnC Loss: 6.2464 | Avg FiC Loss: 2.6755 | Avg CE Loss: 1.5231 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 13.1851 | Avg PrC Loss: 2.7662 | Avg SnC Loss: 6.2286 | Avg FiC Loss: 2.6687 | Avg CE Loss: 1.5216 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 13.1655 | Avg PrC Loss: 2.7643 | Avg SnC Loss: 6.2170 | Avg FiC Loss: 2.6663 | Avg CE Loss: 1.5179 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 13.1446 | Avg PrC Loss: 2.7617 | Avg SnC Loss: 6.2113 | Avg FiC Loss: 2.6622 | Avg CE Loss: 1.5093 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 13.1302 | Avg PrC Loss: 2.7595 | Avg SnC Loss: 6.2065 | Avg FiC Loss: 2.6592 | Avg CE Loss: 1.5049 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 13.1022 | Avg PrC Loss: 2.7543 | Avg SnC Loss: 6.1930 | Avg FiC Loss: 2.6537 | Avg CE Loss: 1.5012 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 13.0828 | Avg PrC Loss: 2.7544 | Avg SnC Loss: 6.1826 | Avg FiC Loss: 2.6518 | Avg CE Loss: 1.4941 | Model Sparsity: 0.95\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 13.0830 | Avg PrC Loss: 2.7551 | Avg SnC Loss: 6.1827 | Avg FiC Loss: 2.6525 | Avg CE Loss: 1.4927 | Model Sparsity: 0.95\n",
      "\n",
      "[BaCP] weights saved!\n",
      "[BaCP TRAINER] Mask generated from current model.\n",
      "[TRAINER] Image size: None\n",
      "[TRAINER] Initialized models\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_pruning.pt\n",
      "[TRAINER] Weights loaded\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.001)\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Finetuning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Current sparsity: 0.9500\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_finetune.pt\n",
      "[LOGGER] Log file created at location: ./log_records/distilbert-base-uncased/wikitext2/bacp_finetune/magnitude_pruning/0.95/run_2.log\n",
      "[TRAINER] Training with mixed precision enabled\n",
      "[TRAINER] Initial model sparsity: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [1/50]: Avg Loss: 3.3557 | Avg Accuracy: 47.64 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 25.275\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [2/50]: Avg Loss: 2.8343 | Avg Accuracy: 48.37 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 24.158\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [3/50]: Avg Loss: 2.7228 | Avg Accuracy: 48.09 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [4/50]: Avg Loss: 2.6384 | Avg Accuracy: 48.58 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.999\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [5/50]: Avg Loss: 2.5753 | Avg Accuracy: 49.02 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.284\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [6/50]: Avg Loss: 2.5316 | Avg Accuracy: 48.75 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.728\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [7/50]: Avg Loss: 2.4776 | Avg Accuracy: 49.15 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.219\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [8/50]: Avg Loss: 2.4404 | Avg Accuracy: 49.20 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.093\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [9/50]: Avg Loss: 2.4044 | Avg Accuracy: 48.86 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.864\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [10/50]: Avg Loss: 2.3655 | Avg Accuracy: 48.83 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [11/50]: Avg Loss: 2.3406 | Avg Accuracy: 49.36 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.393\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [12/50]: Avg Loss: 2.3112 | Avg Accuracy: 49.27 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [13/50]: Avg Loss: 2.2785 | Avg Accuracy: 49.99 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 20.232\n",
      "\n",
      "[TRAINER] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [14/50]: Avg Loss: 2.2555 | Avg Accuracy: 49.32 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [15/50]: Avg Loss: 2.2323 | Avg Accuracy: 49.53 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 20.801\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [16/50]: Avg Loss: 2.2100 | Avg Accuracy: 49.49 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.482\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [17/50]: Avg Loss: 2.1874 | Avg Accuracy: 49.12 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 22.721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [18/50]: Avg Loss: 2.1664 | Avg Accuracy: 49.07 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 22.131\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [19/50]: Avg Loss: 2.1383 | Avg Accuracy: 49.16 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 22.012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [20/50]: Avg Loss: 2.1337 | Avg Accuracy: 49.67 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 20.775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [21/50]: Avg Loss: 2.1021 | Avg Accuracy: 48.74 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 22.685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [22/50]: Avg Loss: 2.0951 | Avg Accuracy: 49.60 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.518\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [23/50]: Avg Loss: 2.0779 | Avg Accuracy: 49.70 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.820\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [24/50]: Avg Loss: 2.0626 | Avg Accuracy: 48.91 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [25/50]: Avg Loss: 2.0437 | Avg Accuracy: 49.49 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 21.999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [26/50]: Avg Loss: 2.0292 | Avg Accuracy: 48.52 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [27/50]: Avg Loss: 2.0138 | Avg Accuracy: 49.62 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 22.143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [28/50]: Avg Loss: 2.0021 | Avg Accuracy: 49.24 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [29/50]: Avg Loss: 1.9758 | Avg Accuracy: 49.37 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.509\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [30/50]: Avg Loss: 1.9709 | Avg Accuracy: 49.11 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.790\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [31/50]: Avg Loss: 1.9564 | Avg Accuracy: 49.43 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 24.129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [32/50]: Avg Loss: 1.9489 | Avg Accuracy: 49.23 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.957\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch [33/50]: Avg Loss: 1.9179 | Avg Accuracy: 49.75 | Model Sparsity: 0.95\n",
      "Avg Perplexity: 23.117\n",
      "\n",
      "[TRAINER] Training stopped. No improvements for 20 epochs.\n",
      "[TRAINER] Loading weights: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.95_bacp_finetune.pt\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Model Sparsity: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING STATISTICS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Performance Metrics:\n",
      "------------------------------\n",
      "  Accuracy:     49.79%\n",
      "  Perplexity:   20.780\n",
      "\n",
      "Model Information:\n",
      "------------------------------\n",
      "  Total Parameters:     66,985,530\n",
      "  Trainable Parameters: 66,985,530\n",
      "  Model Sparsity:       0.9500 (95.00%)\n",
      "\n",
      "Training Configuration:\n",
      "------------------------------\n",
      "  Model:                distilbert-base-uncased\n",
      "  Task:                 wikitext2\n",
      "  Learning Type:        bacp_finetune\n",
      "  Batch Size:           64\n",
      "  Learning Rate:        0.001\n",
      "  Optimizer:            adamw\n",
      "  Epochs:               50\n",
      "\n",
      "System Information:\n",
      "------------------------------\n",
      "  Device:               cuda\n",
      "  Mixed Precision:      True\n",
      "  Workers:              24\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b0727a-3490-4df0-85c7-90234dc43794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b9a9b5-6cea-4d16-b569-006b6c9f89bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINER] Image size: None\n",
      "[TRAINER] Weights loaded successfully\n",
      "[TRAINER] Initialized BaCP models\n",
      "[TRAINER] Optimizer type w/ learning rate: (adamw, 0.001)\n",
      "[TRAINER] No scheduler initialized\n",
      "[TRAINER] Data Initialized for model task: wikitext2\n",
      "[TRAINER] Batch size: 64\n",
      "[TRAINER] Number of dataloders: 3\n",
      "[TRAINER] Pruning initialized\n",
      "[TRAINER] Pruning type: magnitude_pruning\n",
      "[TRAINER] Target sparsity: 0.99\n",
      "[TRAINER] Sparsity scheduler: cubic\n",
      "[TRAINER] Pruning epochs: 5\n",
      "[TRAINER] Current sparsity: 0.0000\n",
      "[TRAINER] Saving model to: ./research/distilbert-base-uncased/wikitext2/distilbert-base-uncased_wikitext2_magnitude_pruning_0.99_bacp_pruning.pt\n",
      "[LOGGER] Log file created at location: ./log_records/distilbert-base-uncased/wikitext2/bacp_pruning/magnitude_pruning/0.99/run_1.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [1/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.483.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: Avg Total Loss: 7.6300 | Avg PrC Loss: 2.6036 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.4892 | Avg CE Loss: 2.5372 | Model Sparsity: 0.4831\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 6.7261 | Avg PrC Loss: 2.2543 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0442 | Avg CE Loss: 2.4276 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 6.6043 | Avg PrC Loss: 2.2453 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0221 | Avg CE Loss: 2.3368 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 6.5156 | Avg PrC Loss: 2.2511 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0215 | Avg CE Loss: 2.2430 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 6.4194 | Avg PrC Loss: 2.2515 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0183 | Avg CE Loss: 2.1496 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 6.3485 | Avg PrC Loss: 2.2651 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0301 | Avg CE Loss: 2.0533 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 6.2753 | Avg PrC Loss: 2.2742 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0377 | Avg CE Loss: 1.9634 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 6.2069 | Avg PrC Loss: 2.2795 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0411 | Avg CE Loss: 1.8863 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 6.1610 | Avg PrC Loss: 2.2869 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0465 | Avg CE Loss: 1.8276 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 6.1099 | Avg PrC Loss: 2.2844 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0440 | Avg CE Loss: 1.7814 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 6.0632 | Avg PrC Loss: 2.2838 | Avg SnC Loss: 0.0000 | Avg FiC Loss: 2.0419 | Avg CE Loss: 1.7375 | Model Sparsity: 0.4831\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [2/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.776.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5]: Avg Total Loss: 10.1804 | Avg PrC Loss: 3.1152 | Avg SnC Loss: 2.0802 | Avg FiC Loss: 3.0693 | Avg CE Loss: 1.9157 | Model Sparsity: 0.7762\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 8.5710 | Avg PrC Loss: 2.6442 | Avg SnC Loss: 1.6421 | Avg FiC Loss: 2.5318 | Avg CE Loss: 1.7529 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 8.4014 | Avg PrC Loss: 2.5889 | Avg SnC Loss: 1.6416 | Avg FiC Loss: 2.4596 | Avg CE Loss: 1.7113 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 8.2973 | Avg PrC Loss: 2.5584 | Avg SnC Loss: 1.6383 | Avg FiC Loss: 2.4173 | Avg CE Loss: 1.6833 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 8.2193 | Avg PrC Loss: 2.5376 | Avg SnC Loss: 1.6347 | Avg FiC Loss: 2.3884 | Avg CE Loss: 1.6586 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 8.1614 | Avg PrC Loss: 2.5236 | Avg SnC Loss: 1.6314 | Avg FiC Loss: 2.3664 | Avg CE Loss: 1.6401 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 8.1246 | Avg PrC Loss: 2.5159 | Avg SnC Loss: 1.6339 | Avg FiC Loss: 2.3561 | Avg CE Loss: 1.6187 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 8.0728 | Avg PrC Loss: 2.5031 | Avg SnC Loss: 1.6302 | Avg FiC Loss: 2.3383 | Avg CE Loss: 1.6011 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 8.0426 | Avg PrC Loss: 2.4969 | Avg SnC Loss: 1.6314 | Avg FiC Loss: 2.3302 | Avg CE Loss: 1.5842 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 8.0034 | Avg PrC Loss: 2.4896 | Avg SnC Loss: 1.6267 | Avg FiC Loss: 2.3185 | Avg CE Loss: 1.5686 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 7.9702 | Avg PrC Loss: 2.4819 | Avg SnC Loss: 1.6249 | Avg FiC Loss: 2.3099 | Avg CE Loss: 1.5535 | Model Sparsity: 0.7762\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [3/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.927.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5]: Avg Total Loss: 13.2674 | Avg PrC Loss: 3.3428 | Avg SnC Loss: 4.6134 | Avg FiC Loss: 3.3389 | Avg CE Loss: 1.9724 | Model Sparsity: 0.9266\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [1/10]: Avg Total Loss: 11.0880 | Avg PrC Loss: 2.9558 | Avg SnC Loss: 3.4342 | Avg FiC Loss: 2.9097 | Avg CE Loss: 1.7883 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [2/10]: Avg Total Loss: 10.7464 | Avg PrC Loss: 2.8784 | Avg SnC Loss: 3.3134 | Avg FiC Loss: 2.8173 | Avg CE Loss: 1.7372 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [3/10]: Avg Total Loss: 10.5806 | Avg PrC Loss: 2.8383 | Avg SnC Loss: 3.2627 | Avg FiC Loss: 2.7685 | Avg CE Loss: 1.7111 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [4/10]: Avg Total Loss: 10.4775 | Avg PrC Loss: 2.8147 | Avg SnC Loss: 3.2402 | Avg FiC Loss: 2.7373 | Avg CE Loss: 1.6853 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [5/10]: Avg Total Loss: 10.3990 | Avg PrC Loss: 2.7950 | Avg SnC Loss: 3.2174 | Avg FiC Loss: 2.7142 | Avg CE Loss: 1.6724 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [6/10]: Avg Total Loss: 10.3342 | Avg PrC Loss: 2.7786 | Avg SnC Loss: 3.2025 | Avg FiC Loss: 2.6941 | Avg CE Loss: 1.6589 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [7/10]: Avg Total Loss: 10.2690 | Avg PrC Loss: 2.7631 | Avg SnC Loss: 3.1848 | Avg FiC Loss: 2.6764 | Avg CE Loss: 1.6446 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [8/10]: Avg Total Loss: 10.2589 | Avg PrC Loss: 2.7637 | Avg SnC Loss: 3.1908 | Avg FiC Loss: 2.6733 | Avg CE Loss: 1.6311 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [9/10]: Avg Total Loss: 10.2104 | Avg PrC Loss: 2.7523 | Avg SnC Loss: 3.1814 | Avg FiC Loss: 2.6592 | Avg CE Loss: 1.6175 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining Epoch [10/10]: Avg Total Loss: 10.1689 | Avg PrC Loss: 2.7424 | Avg SnC Loss: 3.1697 | Avg FiC Loss: 2.6474 | Avg CE Loss: 1.6094 | Model Sparsity: 0.9266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch [4/5]:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Pruner] Cubic Sparsity ratio increased to 0.982.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5]: Avg Total Loss: 19.2737 | Avg PrC Loss: 3.8603 | Avg SnC Loss: 9.3560 | Avg FiC Loss: 3.8744 | Avg CE Loss: 2.1830 | Model Sparsity: 0.9821\n",
      "\n",
      "[BaCP] weights saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining epoch [1/10]:  65%|██████▍   | 46/71 [00:35<00:19,  1.25it/s, Loss=1.91, PrC Loss=3.35, SnC Loss=6.32, FiC Loss=3.35, CE Loss=1.91]"
     ]
    }
   ],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"magnitude_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f87345-2c84-4316-b7ba-d5f229fa60fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SNIP-it Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efccf65a-d263-4ddc-a992-866ab981499b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d93344e-6c12-42a5-8b96-e783c52a4169",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a4a15d8-68c5-484f-bdc7-5bed39634289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"snip_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801a1094-fa19-4155-b57f-ef07f27709c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WandA Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffe7d48-7c1b-4dbd-acde-ec958238105f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_LOW,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab59c27b-661f-418b-96dc-d21d351446cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_MID,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87f7f4d8-1802-4c32-b7d6-eff885d7d8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trained_model_path = f\"./research/{MODEL_NAME}/{MODEL_TASK}/{MODEL_NAME}_{MODEL_TASK}_baseline.pt\"\n",
    "\n",
    "bacp_training_args = BaCPTrainingArguments(\n",
    "    model_name=MODEL_NAME,\n",
    "    model_task=MODEL_TASK,\n",
    "    batch_size=BATCH_SIZE_LLM,   \n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=\"wanda_pruning\",\n",
    "    target_sparsity=TARGET_SPARSITY_HIGH,\n",
    "    sparsity_scheduler='cubic',\n",
    "    finetuned_weights=trained_model_path,\n",
    "    learning_type='bacp_pruning',\n",
    "    db=False,\n",
    ")\n",
    "bacp_trainer = BaCPTrainer(bacp_training_args)\n",
    "if TRAIN:\n",
    "    bacp_trainer.train()\n",
    "    \n",
    "# Finetuning Phase\n",
    "bacp_trainer.generate_mask_from_model()\n",
    "training_args = TrainingArguments(\n",
    "    model_name=bacp_trainer.model_name,\n",
    "    model_task=bacp_trainer.model_task,\n",
    "    batch_size=bacp_trainer.batch_size,\n",
    "    optimizer_type_and_lr=('adamw', 1e-3),\n",
    "    pruning_type=bacp_trainer.pruning_type,\n",
    "    target_sparsity=bacp_trainer.target_sparsity,\n",
    "    finetuned_weights=bacp_trainer.save_path,\n",
    "    epochs=50,\n",
    "    pruner=bacp_trainer.get_pruner(),\n",
    "    finetune=True,\n",
    "    learning_type=\"bacp_finetune\",\n",
    "    db=False\n",
    ")\n",
    "trainer = Trainer(training_args)\n",
    "if TRAIN:\n",
    "    trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print_statistics(metrics, trainer)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6215900699327025,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DistilBERT_wikitext2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
